{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ontology Extension for nbdev\n",
    "\n",
    "> An extension to nbdev for ontology and vocabulary development using notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from nbdev.process import *\n",
    "from nbdev.maker import *\n",
    "from execnb.nbio import read_nb, NbCell  # Updated import\n",
    "from rdflib import Graph, URIRef, Literal, Namespace\n",
    "from pathlib import Path\n",
    "from fastcore.test import *\n",
    "from fastcore.script import call_parse  # Add this import\n",
    "import json\n",
    "import re\n",
    "from markdown_it import MarkdownIt\n",
    "from markdown_it.tree import SyntaxTreeNode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OntologyGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class OntologyGraph:\n",
    "    \"Builds and manages the RDF graph for ontology content\"\n",
    "    def __init__(self):\n",
    "        self.graph = Graph()\n",
    "        \n",
    "    def add_cell_content(self, cell):\n",
    "        \"Add content from a cell to the graph\"\n",
    "        if cell.cell_type == 'markdown' and 'export_mdld' in cell.directives_:\n",
    "            self._add_markdown_ld(cell.source)\n",
    "        elif cell.cell_type == 'code':\n",
    "            if 'export_ttl' in cell.directives_:\n",
    "                self._add_turtle(cell.source)\n",
    "            elif 'export_jsonld' in cell.directives_:\n",
    "                self._add_jsonld(cell.source)\n",
    "    \n",
    "    def _add_markdown_ld(self, text):\n",
    "        # Use our local function to parse and add\n",
    "        triples, prefixes, base_uri = markdown_ld_to_triples(text)\n",
    "        # Add prefixes to graph\n",
    "        for prefix, uri in prefixes.items():\n",
    "            self.graph.bind(prefix, Namespace(uri))\n",
    "        # Add base URI if provided\n",
    "        if base_uri:\n",
    "            self.graph.base = base_uri\n",
    "        # Add triples to graph\n",
    "        for s, p, o in triples:\n",
    "            # Handle URIs vs literals\n",
    "            if s.startswith('<') and s.endswith('>'): s = s[1:-1]\n",
    "            if p.startswith('<') and p.endswith('>'): p = p[1:-1]\n",
    "            \n",
    "            if o.startswith('\"'):\n",
    "                # It's a literal\n",
    "                obj_val = Literal(o.strip('\"'))\n",
    "            elif o.isdigit():\n",
    "                obj_val = Literal(int(o))\n",
    "            else:\n",
    "                if o.startswith('<') and o.endswith('>'): o = o[1:-1]\n",
    "                obj_val = URIRef(o)\n",
    "                \n",
    "            self.graph.add((URIRef(s), URIRef(p), obj_val))\n",
    "\n",
    "            \n",
    "    def _add_turtle(self, text):\n",
    "        # Parse Turtle directly into the graph\n",
    "        self.graph.parse(data=text, format=\"turtle\")\n",
    "    \n",
    "    def _add_jsonld(self, text):\n",
    "        # Parse JSON-LD directly into the graph\n",
    "        self.graph.parse(data=text, format=\"json-ld\")\n",
    "    \n",
    "    def serialize(self, format=\"turtle\"):\n",
    "        \"Serialize the graph to the specified format\"\n",
    "        if format == \"jsonld-context\":\n",
    "            # Generate a JSON-LD context from the graph\n",
    "            # This would need custom implementation\n",
    "            return self._generate_jsonld_context()\n",
    "        else:\n",
    "            return self.graph.serialize(format=format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic functionality of OntologyGraph\n",
    "test_graph = OntologyGraph()\n",
    "test_turtle = \"\"\"\n",
    "@prefix ex: <http://example.org/> .\n",
    "ex:Subject ex:predicate \"object\" .\n",
    "\"\"\"\n",
    "test_graph._add_turtle(test_turtle)\n",
    "test_eq(len(test_graph.graph), 1)\n",
    "serialized = test_graph.serialize(format=\"turtle\")\n",
    "test_eq(\"ex:Subject\" in serialized, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ontology Maker\n",
    "OntologyMaker class that will handle exporting the graph to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class OntologyMaker:\n",
    "    \"Export ontology graph to various formats\"\n",
    "    def __init__(self, dest, name, format=\"turtle\"):\n",
    "        self.dest = Path(dest)\n",
    "        self.name = name\n",
    "        self.format = format\n",
    "        self.dest.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    def make(self, graph):\n",
    "        \"Export the graph to the specified format\"\n",
    "        # Determine file extension based on format\n",
    "        ext = self._get_extension()\n",
    "        filename = f\"{self.name}.{ext}\"\n",
    "        filepath = self.dest / filename\n",
    "        \n",
    "        # Serialize and write to file\n",
    "        serialized = graph.serialize(format=self.format)\n",
    "        with open(filepath, 'w') as f:\n",
    "            f.write(serialized)\n",
    "        \n",
    "        return filepath\n",
    "    \n",
    "    def _get_extension(self):\n",
    "        \"Get file extension based on format\"\n",
    "        format_map = {\n",
    "            \"turtle\": \"ttl\",\n",
    "            \"xml\": \"rdf\",\n",
    "            \"json-ld\": \"jsonld\",\n",
    "            \"n3\": \"n3\",\n",
    "            \"nt\": \"nt\",\n",
    "            \"nquads\": \"nq\"\n",
    "        }\n",
    "        return format_map.get(self.format, self.format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def extract_prefixes(ast):\n",
    "    \"\"\"Extract prefixes and base URI from a Markdown-ld AST.\"\"\"\n",
    "    prefixes = {}\n",
    "    base_uri = None\n",
    "    \n",
    "    # Look for level-1 heading followed by a code block\n",
    "    for i, node in enumerate(ast.children):\n",
    "        if node.type == 'heading' and node.tag == 'h1':\n",
    "            # Check if next node is a code block\n",
    "            if i+1 < len(ast.children) and ast.children[i+1].type == 'code':\n",
    "                code_block = ast.children[i+1]\n",
    "                # Parse the code block content for prefixes\n",
    "                for line in code_block.content.split('\\n'):\n",
    "                    line = line.strip()\n",
    "                    if line.startswith('@prefix'):\n",
    "                        # Extract prefix and URI\n",
    "                        parts = line.split(' ', 2)\n",
    "                        if len(parts) >= 3:\n",
    "                            prefix = parts[1].rstrip(':')\n",
    "                            uri = parts[2].rstrip(' .')\n",
    "                            if uri.startswith('<') and uri.endswith('>'): \n",
    "                                uri = uri[1:-1]\n",
    "                            prefixes[prefix] = uri\n",
    "                    elif line.startswith('@base'):\n",
    "                        # Extract base URI\n",
    "                        parts = line.split(' ', 1)\n",
    "                        if len(parts) >= 2:\n",
    "                            uri = parts[1].rstrip(' .')\n",
    "                            if uri.startswith('<') and uri.endswith('>'): \n",
    "                                uri = uri[1:-1]\n",
    "                            base_uri = uri\n",
    "    \n",
    "    return prefixes, base_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def extract_triples(ast):\n",
    "    \"\"\"Extract RDF triples from a Markdown-ld AST.\"\"\"\n",
    "    triples = []\n",
    "    current_subject = None\n",
    "    current_predicate = None\n",
    "    \n",
    "    # Helper function to extract text from inline code\n",
    "    def extract_code_value(node):\n",
    "        for child in node.children:\n",
    "            if child.type == 'inline':\n",
    "                for inline_child in child.children:\n",
    "                    if inline_child.type == 'code_inline':\n",
    "                        return inline_child.content\n",
    "        return None\n",
    "    \n",
    "    # Process each node in the AST\n",
    "    for node in ast.children:\n",
    "        # Handle level-2 headings (subjects)\n",
    "        if node.type == 'heading' and node.tag == 'h2':\n",
    "            # The next node should be a paragraph with inline code\n",
    "            if node.next_sibling and node.next_sibling.type == 'paragraph':\n",
    "                current_subject = extract_code_value(node.next_sibling)\n",
    "                current_predicate = None\n",
    "        \n",
    "        # Handle level-3 headings (predicates)\n",
    "        elif node.type == 'heading' and node.tag == 'h3':\n",
    "            if node.next_sibling and node.next_sibling.type == 'paragraph':\n",
    "                current_predicate = extract_code_value(node.next_sibling)\n",
    "        \n",
    "        # Handle bullet lists (objects)\n",
    "        elif node.type == 'bullet_list' and current_subject and current_predicate:\n",
    "            for list_item in node.children:\n",
    "                if list_item.type == 'list_item':\n",
    "                    # Extract object from list item\n",
    "                    obj = None\n",
    "                    for paragraph in list_item.children:\n",
    "                        if paragraph.type == 'paragraph':\n",
    "                            obj = extract_code_value(paragraph)\n",
    "                    \n",
    "                    if obj:\n",
    "                        triples.append((current_subject, current_predicate, obj))\n",
    "    \n",
    "    return triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def markdown_ld_to_triples(markdown_text):\n",
    "    \"\"\"Convert Markdown-LD text to RDF triples.\"\"\"\n",
    "    from markdown_it import MarkdownIt\n",
    "    from markdown_it.tree import SyntaxTreeNode\n",
    "    \n",
    "    md = MarkdownIt()\n",
    "    tokens = md.parse(markdown_text)\n",
    "    ast = SyntaxTreeNode(tokens)\n",
    "    triples = extract_triples(ast)\n",
    "    prefixes, base_uri = extract_prefixes(ast)\n",
    "    return triples, prefixes, base_uri\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with Markdown-LD\n",
    "test_mdld = \"\"\"\n",
    "# Test Ontology\n",
    "\n",
    "```\n",
    "@prefix ex: <http://example.org/> .\n",
    "@base <http://example.org/base/> .\n",
    "```\n",
    "\n",
    "## TestClass\n",
    "\n",
    "`ex:TestClass`\n",
    "\n",
    "```\n",
    "ex:TestClass a owl:Class .\n",
    "```\n",
    "\n",
    "### has property\n",
    "\n",
    "`ex:hasProperty`\n",
    "\n",
    "- `\"test value\"`\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graph = OntologyGraph()\n",
    "test_graph._add_markdown_ld(test_mdld)\n",
    "test_eq(len(test_graph.graph) > 0, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class OntologyProcessor(Processor):\n",
    "    \"Process cells with ontology directives\"\n",
    "    def __init__(self, nb):\n",
    "        super().__init__(nb)\n",
    "        self.graph = OntologyGraph()\n",
    "        \n",
    "    def _export_owl_(self, cell):\n",
    "        \"Process cells marked with #| export_owl\"\n",
    "        self.graph.add_cell_content(cell)\n",
    "        \n",
    "    def _export_ttl_(self, cell):\n",
    "        \"Process cells marked with #| export_ttl\"\n",
    "        self.graph.add_cell_content(cell)\n",
    "        \n",
    "    def _export_mdld_(self, cell):\n",
    "        \"Process cells marked with #| export_mdld\"\n",
    "        self.graph.add_cell_content(cell)\n",
    "        \n",
    "    def get_graph(self):\n",
    "        \"Return the constructed RDF graph\"\n",
    "        return self.graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test OntologyProcessor with a notebook cell\n",
    "import nbformat\n",
    "from execnb.nbio import NbCell\n",
    "\n",
    "# Create a test cell using nbformat first\n",
    "nb_cell = nbformat.v4.new_code_cell(source=\"\"\"\n",
    "@prefix ex: <http://example.org/> .\n",
    "ex:TestSubject ex:testPredicate \"test object\" .\n",
    "\"\"\")\n",
    "\n",
    "# Now create a NbCell with the required arguments\n",
    "test_cell = NbCell(0, nb_cell)  # idx=0, cell=nb_cell\n",
    "test_cell.directives_ = {'export_ttl': []}\n",
    "\n",
    "# Create a processor and process the cell\n",
    "processor = OntologyProcessor(None)  # nb parameter not used in this test\n",
    "processor._export_ttl_(test_cell)\n",
    "\n",
    "# Check if the triple was added to the graph\n",
    "test_eq(len(processor.graph.graph), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test OntologyMaker\n",
    "import tempfile\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    # Create a graph with a test triple\n",
    "    test_graph = OntologyGraph()\n",
    "    test_graph._add_turtle(\"\"\"\n",
    "    @prefix ex: <http://example.org/> .\n",
    "    ex:TestSubject ex:testPredicate \"test object\" .\n",
    "    \"\"\")\n",
    "    \n",
    "    # Export the graph\n",
    "    maker = OntologyMaker(tmpdir, \"test_ontology\")\n",
    "    filepath = maker.make(test_graph.graph)  # Note: We pass the RDFLib graph, not the OntologyGraph\n",
    "    \n",
    "    # Check that the file was created and contains our triple\n",
    "    test_eq(Path(filepath).exists(), True)\n",
    "    with open(filepath, 'r') as f:\n",
    "        content = f.read()\n",
    "    test_eq(\"TestSubject\" in content, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def process_ontology_notebook(nb_path, output_dir=None, formats=None):\n",
    "    \"\"\"Process a notebook and export ontology content to files.\n",
    "    \n",
    "    Args:\n",
    "        nb_path: Path to the notebook\n",
    "        output_dir: Directory to output the ontology files (defaults to same dir as notebook)\n",
    "        formats: List of output formats (defaults to [\"turtle\"])\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping formats to output file paths\n",
    "    \"\"\"\n",
    "    # Set defaults\n",
    "    if formats is None: formats = [\"turtle\"]\n",
    "    nb_path = Path(nb_path)\n",
    "    if output_dir is None: output_dir = nb_path.parent\n",
    "    \n",
    "    # Read the notebook\n",
    "    nb = read_nb(nb_path)\n",
    "    \n",
    "    # Process the notebook\n",
    "    processor = OntologyProcessor(nb)\n",
    "    for cell in nb.cells:\n",
    "        if not hasattr(cell, 'directives_'):\n",
    "            cell.directives_ = extract_directives(cell)\n",
    "        \n",
    "        # Use 'in' instead of 'get' to check for directive presence\n",
    "        if 'export_ttl' in cell.directives_:\n",
    "            processor._export_ttl_(cell)\n",
    "        elif 'export_owl' in cell.directives_:\n",
    "            processor._export_owl_(cell)\n",
    "        elif 'export_mdld' in cell.directives_:\n",
    "            processor._export_mdld_(cell)\n",
    "    \n",
    "    # Get the graph\n",
    "    graph = processor.graph.graph\n",
    "    \n",
    "    # Export to each format\n",
    "    results = {}\n",
    "    for fmt in formats:\n",
    "        maker = OntologyMaker(output_dir, nb_path.stem, format=fmt)\n",
    "        filepath = maker.make(graph)\n",
    "        results[fmt] = filepath\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File content:\n",
      "@prefix ex: <http://example.org/> .\n",
      "\n",
      "ex:TestSubject ex:testPredicate \"test object\" .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test process_ontology_notebook with the correct string to check\n",
    "import nbformat\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    # Create a test notebook\n",
    "    nb = nbformat.v4.new_notebook()\n",
    "    \n",
    "    # Add a cell with Turtle content\n",
    "    cell = nbformat.v4.new_code_cell(source=\"\"\"#| export_ttl\n",
    "@prefix ex: <http://example.org/> .\n",
    "ex:TestSubject ex:testPredicate \"test object\" .\n",
    "\"\"\")\n",
    "    nb.cells.append(cell)\n",
    "    \n",
    "    # Write the notebook to a file\n",
    "    nb_path = Path(tmpdir) / \"test_ontology.ipynb\"\n",
    "    with open(nb_path, 'w') as f:\n",
    "        nbformat.write(nb, f)\n",
    "    \n",
    "    # Process the notebook\n",
    "    results = process_ontology_notebook(nb_path)\n",
    "    \n",
    "    # Check that the output file was created\n",
    "    test_eq(len(results), 1)\n",
    "    test_eq(\"turtle\" in results, True)\n",
    "    test_eq(Path(results[\"turtle\"]).exists(), True)\n",
    "    \n",
    "    # Check the content\n",
    "    with open(results[\"turtle\"], 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Print the content for debugging\n",
    "    print(\"File content:\")\n",
    "    print(content)\n",
    "    \n",
    "    # Check for the correct string pattern\n",
    "    test_eq(\"ex:TestSubject\" in content, True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@call_parse\n",
    "def nbvoc_export(\n",
    "    path:str=None, # Path to notebook or directory containing notebooks (default: current dir)\n",
    "    recursive:bool=True, # Search recursively in directories\n",
    "    formats:str='turtle', # Output formats (comma-separated list)\n",
    "    output_dir:str=None, # Output directory (default: ontology_path from settings.ini)\n",
    "):\n",
    "    \"Export ontology content from notebooks\"\n",
    "    from nbdev.config import get_config\n",
    "    \n",
    "    path = Path(path or '.')\n",
    "    format_list = formats.split(',')\n",
    "    \n",
    "    # Use ontology_path from settings.ini or default to 'ontology'\n",
    "    if output_dir is None:\n",
    "        try:\n",
    "            output_dir = get_config().get('ontology_path', 'ontology')\n",
    "        except:\n",
    "            output_dir = 'ontology'\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Process a single notebook\n",
    "    if path.is_file() and path.suffix == '.ipynb':\n",
    "        # Preserve directory structure relative to current directory\n",
    "        rel_path = path.relative_to(Path.cwd())\n",
    "        # Get the parent directory for the output\n",
    "        rel_dir = rel_path.parent\n",
    "        # Create parent directory if needed\n",
    "        notebook_output_dir = output_path / rel_dir\n",
    "        notebook_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        results = process_ontology_notebook(path, notebook_output_dir, format_list)\n",
    "        print(f\"Exported {path.name} to: {', '.join(str(p) for p in results.values())}\")\n",
    "        return\n",
    "    \n",
    "    # Process a directory of notebooks\n",
    "    if path.is_dir():\n",
    "        pattern = '**/*.ipynb' if recursive else '*.ipynb'\n",
    "        notebooks = list(path.glob(pattern))\n",
    "        if not notebooks:\n",
    "            print(f\"No notebooks found in {path}\")\n",
    "            return\n",
    "            \n",
    "        for nb_path in notebooks:\n",
    "            try:\n",
    "                # Preserve directory structure relative to search path\n",
    "                rel_path = nb_path.relative_to(path)\n",
    "                # Get the parent directory for the output\n",
    "                rel_dir = rel_path.parent\n",
    "                # Create parent directory if needed\n",
    "                notebook_output_dir = output_path / rel_dir\n",
    "                notebook_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "                \n",
    "                results = process_ontology_notebook(nb_path, notebook_output_dir, format_list)\n",
    "                print(f\"Exported {nb_path.name} to: {', '.join(str(p) for p in results.values())}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {nb_path}: {e}\")\n",
    "        \n",
    "        return\n",
    "    \n",
    "    print(f\"Path not found or not a notebook: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing export with directory structure preservation:\n",
      "Exported ontology1.ipynb to: ontology/module1/ontology1.ttl\n",
      "Exported ontology2.ipynb to: ontology/module2/submodule/ontology2.ttl\n",
      "Checking if /var/folders/qp/nh72_hpd4876lr8t6k44015c0000gn/T/tmpvcvvctss/ontology/module1/ontology1.ttl exists...\n",
      "Checking if /var/folders/qp/nh72_hpd4876lr8t6k44015c0000gn/T/tmpvcvvctss/ontology/module2/submodule/ontology2.ttl exists...\n",
      "Directory structure test passed!\n"
     ]
    }
   ],
   "source": [
    "# Test the CLI function with directory structure preservation\n",
    "import tempfile\n",
    "import nbformat\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    tmp_path = Path(tmpdir)\n",
    "    \n",
    "    # Create a directory structure\n",
    "    notebooks_dir = tmp_path / \"notebooks\"\n",
    "    notebooks_dir.mkdir()\n",
    "    \n",
    "    subdir1 = notebooks_dir / \"module1\"\n",
    "    subdir1.mkdir()\n",
    "    \n",
    "    subdir2 = notebooks_dir / \"module2\" / \"submodule\"\n",
    "    subdir2.mkdir(parents=True)\n",
    "    \n",
    "    # Create test notebooks in different directories\n",
    "    nb1 = nbformat.v4.new_notebook()\n",
    "    cell1 = nbformat.v4.new_code_cell(source=\"\"\"#| export_ttl\n",
    "@prefix ex: <http://example.org/> .\n",
    "ex:Module1 ex:hasProperty \"value1\" .\n",
    "\"\"\")\n",
    "    nb1.cells.append(cell1)\n",
    "    \n",
    "    nb2 = nbformat.v4.new_notebook()\n",
    "    cell2 = nbformat.v4.new_code_cell(source=\"\"\"#| export_ttl\n",
    "@prefix ex: <http://example.org/> .\n",
    "ex:Module2 ex:hasProperty \"value2\" .\n",
    "\"\"\")\n",
    "    nb2.cells.append(cell2)\n",
    "    \n",
    "    # Write notebooks to their respective directories\n",
    "    nb_path1 = subdir1 / \"ontology1.ipynb\"\n",
    "    with open(nb_path1, 'w') as f:\n",
    "        nbformat.write(nb1, f)\n",
    "    \n",
    "    nb_path2 = subdir2 / \"ontology2.ipynb\"\n",
    "    with open(nb_path2, 'w') as f:\n",
    "        nbformat.write(nb2, f)\n",
    "    \n",
    "    # Create output directory\n",
    "    ontology_dir = tmp_path / \"ontology\"\n",
    "    \n",
    "    # Run the export function\n",
    "    print(\"\\nTesting export with directory structure preservation:\")\n",
    "    current_dir = os.getcwd()\n",
    "    try:\n",
    "        # Change to the temp directory to test relative paths\n",
    "        os.chdir(tmp_path)\n",
    "        nbvoc_export(\"notebooks\", output_dir=\"ontology\", formats=\"turtle\")\n",
    "    finally:\n",
    "        # Change back to original directory\n",
    "        os.chdir(current_dir)\n",
    "    \n",
    "    # Check output directory structure\n",
    "    expected_files = [\n",
    "        ontology_dir / \"module1\" / \"ontology1.ttl\",\n",
    "        ontology_dir / \"module2\" / \"submodule\" / \"ontology2.ttl\"\n",
    "    ]\n",
    "    \n",
    "    for file_path in expected_files:\n",
    "        print(f\"Checking if {file_path} exists...\")\n",
    "        test_eq(file_path.exists(), True)\n",
    "        \n",
    "        # Verify content\n",
    "        content = file_path.read_text()\n",
    "        if \"Module1\" in content:\n",
    "            test_eq(\"ex:Module1\" in content, True)\n",
    "        elif \"Module2\" in content:\n",
    "            test_eq(\"ex:Module2\" in content, True)\n",
    "    \n",
    "    print(\"Directory structure test passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing export with directory structure preservation:\n",
      "Exported ontology1.ipynb to: ontology/module1/ontology1.ttl\n",
      "Exported ontology2.ipynb to: ontology/module2/submodule/ontology2.ttl\n",
      "Checking if /var/folders/qp/nh72_hpd4876lr8t6k44015c0000gn/T/tmpeowe_1xp/ontology/module1/ontology1.ttl exists...\n",
      "Checking if /var/folders/qp/nh72_hpd4876lr8t6k44015c0000gn/T/tmpeowe_1xp/ontology/module2/submodule/ontology2.ttl exists...\n",
      "Directory structure test passed!\n"
     ]
    }
   ],
   "source": [
    "# Test the CLI function with directory structure preservation\n",
    "import tempfile\n",
    "import nbformat\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    tmp_path = Path(tmpdir)\n",
    "    \n",
    "    # Create a directory structure\n",
    "    notebooks_dir = tmp_path / \"notebooks\"\n",
    "    notebooks_dir.mkdir()\n",
    "    \n",
    "    subdir1 = notebooks_dir / \"module1\"\n",
    "    subdir1.mkdir()\n",
    "    \n",
    "    subdir2 = notebooks_dir / \"module2\" / \"submodule\"\n",
    "    subdir2.mkdir(parents=True)\n",
    "    \n",
    "    # Create test notebooks in different directories\n",
    "    nb1 = nbformat.v4.new_notebook()\n",
    "    cell1 = nbformat.v4.new_code_cell(source=\"\"\"#| export_ttl\n",
    "@prefix ex: <http://example.org/> .\n",
    "ex:Module1 ex:hasProperty \"value1\" .\n",
    "\"\"\")\n",
    "    nb1.cells.append(cell1)\n",
    "    \n",
    "    nb2 = nbformat.v4.new_notebook()\n",
    "    cell2 = nbformat.v4.new_code_cell(source=\"\"\"#| export_ttl\n",
    "@prefix ex: <http://example.org/> .\n",
    "ex:Module2 ex:hasProperty \"value2\" .\n",
    "\"\"\")\n",
    "    nb2.cells.append(cell2)\n",
    "    \n",
    "    # Write notebooks to their respective directories\n",
    "    nb_path1 = subdir1 / \"ontology1.ipynb\"\n",
    "    with open(nb_path1, 'w') as f:\n",
    "        nbformat.write(nb1, f)\n",
    "    \n",
    "    nb_path2 = subdir2 / \"ontology2.ipynb\"\n",
    "    with open(nb_path2, 'w') as f:\n",
    "        nbformat.write(nb2, f)\n",
    "    \n",
    "    # Create output directory\n",
    "    ontology_dir = tmp_path / \"ontology\"\n",
    "    \n",
    "    # Run the export function\n",
    "    print(\"\\nTesting export with directory structure preservation:\")\n",
    "    current_dir = os.getcwd()\n",
    "    try:\n",
    "        # Change to the temp directory to test relative paths\n",
    "        os.chdir(tmp_path)\n",
    "        nbvoc_export(\"notebooks\", output_dir=\"ontology\", formats=\"turtle\")\n",
    "    finally:\n",
    "        # Change back to original directory\n",
    "        os.chdir(current_dir)\n",
    "    \n",
    "    # Check output directory structure - updated paths\n",
    "    expected_files = [\n",
    "        ontology_dir / \"module1\" / \"ontology1.ttl\",\n",
    "        ontology_dir / \"module2\" / \"submodule\" / \"ontology2.ttl\"\n",
    "    ]\n",
    "    \n",
    "    for file_path in expected_files:\n",
    "        print(f\"Checking if {file_path} exists...\")\n",
    "        test_eq(file_path.exists(), True)\n",
    "        \n",
    "        # Verify content\n",
    "        content = file_path.read_text()\n",
    "        if \"Module1\" in str(file_path):\n",
    "            test_eq(\"ex:Module1\" in content, True)\n",
    "        elif \"Module2\" in str(file_path):\n",
    "            test_eq(\"ex:Module2\" in content, True)\n",
    "    \n",
    "    print(\"Directory structure test passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
